{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snl28/HCJ/blob/main/Class_1_LatentSpace_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Name: Sunil John\n",
        "## SEAS 8525 Computer Vision and Generative AI\n",
        "## Homework 1\n",
        "\n",
        "The notebook implements an autoencoder using PyTorch on the MNIST dataset. The encoder compresses each 28×28 grayscale digit image into a smaller latent representation, while the decoder reconstructs the original image from this compressed form. The training loop minimizes reconstruction loss, teaching the model to capture meaningful features in the latent space. After training,\n",
        "the notebook visualizes both original and reconstructed digits, while fine details may blur slightly, the autoencoder effectively preserves the overall digit structure, proving that the latent space captures essential information. This latent representation is then printed and explored, showing that the model can meaningfully encode high-dimensional image data into compact vectors, where similar digits cluster together in the learned space.\n",
        "\n",
        "The notebook results include both original images from MNIST and their reconstructed versions. Only see the original MNIST images, but not the reconstructed outputs from the autoencoder. At the visualization stage, the code plots only the original batch of images using imshow(vutils.make_grid()), but the part that should pass images through the trained autoencoder\n",
        "and then display the reconstructions is missing?"
      ],
      "metadata": {
        "id": "q_25q2eEkFe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code defines and utilizes an Autoencoder neural network for the MNIST dataset. Here's a detailed explanation:\n",
        "\n",
        "- **Autoencoder Class**: This class creates a neural network with encoder and decoder components. The encoder compresses the input image into a lower-dimensional latent space, and the decoder reconstructs the image from this latent space. The network is structured to flatten and process 28x28 grayscale MNIST images.\n",
        "\n",
        "- **Data Loading**: Utilizes PyTorch's DataLoader to efficiently load the MNIST dataset, applying transformations to normalize the images.\n",
        "\n",
        "- **Training Loop**: Iterates over the training dataset, feeding batches of images through the model, calculating the reconstruction loss, and updating the model's weights to minimize this loss, effectively learning to compress and reconstruct the input images.\n",
        "\n",
        "- **Visualization**: After training, the script visualizes a batch of original images and their reconstructions from the autoencoder. It also prints the latent space representations, showcasing what the model has learned to encode.\n",
        "\n",
        "- **Utility Functions**: Includes `imshow` for displaying tensors as images. It unnormalizes the data and uses Matplotlib to plot them.\n",
        "\n",
        "This script encapsulates the end-to-end process of training an autoencoder on the MNIST dataset, visualizing the results, and examining the learned latent space."
      ],
      "metadata": {
        "id": "MXble1sY6GuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "from torch.cuda.amp import GradScaler, autocast"
      ],
      "metadata": {
        "id": "QTp8sYxwBg-1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " The `Autoencoder` class inherits from `nn.Module`, a base class for all neural network modules in PyTorch. Here's a breakdown of the code and its functionality:\n",
        "\n",
        "### Initialization Method (`__init__`)\n",
        "- **Parameters**: The `__init__` method accepts a single parameter `encoding_dim`, which specifies the size of the latent space where the input data is compressed.\n",
        "- **Encoder**: The encoder part of the autoencoder is designed to compress the input data (in this case, an image) into a lower-dimensional representation called the latent space. It consists of a sequence of layers:\n",
        "  - `nn.Linear(28 * 28, 128)`: This layer flattens the input image (assumed to be 28x28 pixels, typical for MNIST dataset images) into a one-dimensional array and performs a linear transformation to reduce its dimension to 128.\n",
        "  - `nn.ReLU(True)`: A Rectified Linear Unit (ReLU) activation function is applied to introduce non-linearity, helping the model learn complex patterns.\n",
        "  - Another `nn.Linear(128, encoding_dim)`: Further reduces the dimension from 128 to the specified `encoding_dim`.\n",
        "  - Another `nn.ReLU(True)`: Another ReLU activation for non-linearity.\n",
        "- **Decoder**: The decoder part reconstructs the original input data from the compressed representation. It mirrors the encoder structure but in reverse, aiming to expand the compressed data back to its original shape:\n",
        "  - `nn.Linear(encoding_dim, 128)`: Expands the compressed data from `encoding_dim` back to 128.\n",
        "  - `nn.ReLU(True)`: Applies ReLU activation.\n",
        "  - `nn.Linear(128, 28 * 28)`: Transforms the data from 128 back to the flattened image size of 784 (28x28).\n",
        "  - `nn.Sigmoid()`: Applies a sigmoid activation function to ensure the output values are between 0 and 1, suitable for image data where pixel values typically fall within this range.\n",
        "\n",
        "### Forward Method (`forward`)\n",
        "- **Parameter**: The `forward` method defines how the input `x` flows through the network.\n",
        "- **Process**:\n",
        "  - `x.view(-1, 28*28)`: First, the input `x` is reshaped into a one-dimensional array (flattened) if not already done.\n",
        "  - `self.encoder(x)`: The flattened `x` is then passed through the encoder.\n",
        "  - `self.decoder(x)`: The output from the encoder, which is the compressed representation, is fed into the decoder.\n",
        "- **Output**: The final output is reshaped back to the original image dimensions (`-1, 1, 28, 28`), where `-1` is a placeholder that automatically adjusts based on the batch size.\n"
      ],
      "metadata": {
        "id": "Lth1Gula4Sz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder: compresses the image into a lower-dimensional latent space\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 128),  # Flatten the image and then linearly transform it\n",
        "            nn.ReLU(True),  # Non-linear activation function\n",
        "            nn.Linear(128, encoding_dim),  # Linear transformation to the encoding dimension\n",
        "            nn.ReLU(True)  # Non-linear activation function\n",
        "        )\n",
        "        # Decoder: reconstructs the image from the latent space\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 128),  # Linearly transforms the encoding\n",
        "            nn.ReLU(True),  # Non-linear activation function\n",
        "            nn.Linear(128, 28 * 28),  # Transforms back to original image shape\n",
        "            nn.Sigmoid()  # Sigmoid activation to output values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x.view(-1, 28*28))  # Encode the input image\n",
        "        x = self.decoder(x)  # Decode the encoded image\n",
        "        return x.view(-1, 1, 28, 28)  # Reshape to the original image dimensions"
      ],
      "metadata": {
        "id": "wWlX07fEBjL8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def imshow(img):\n",
        "    img = img.cpu() / 2 + 0.5  # Unnormalize the image\n",
        "    npimg = img.numpy()  # Convert the tensor to a numpy array\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # Reshape and display the image\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yP7S8PazBoD5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform: converts images to PyTorch tensors and normalizes them\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n"
      ],
      "metadata": {
        "id": "yJAgqh0TBrMa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset loading\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "cQpswSgnBw4E",
        "outputId": "44b711f1-e15d-4e6f-e24f-85264de45111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.03MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 133kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.26MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.18MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Autoencoder(encoding_dim=64).to(device)\n",
        "criterion = nn.MSELoss()  # Loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Optimizer"
      ],
      "metadata": {
        "id": "W8NemtCtB35p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_loader:\n",
        "        img, _ = data\n",
        "        img = img.to(device)\n",
        "\n",
        "        with autocast():\n",
        "            output = model(img)\n",
        "            loss = criterion(output, img)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "    print('Epoch [{}/{}], Loss:{:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GRk72snY6LPD",
        "outputId": "1eb099ef-1711-4d1c-bdb7-a92de2a93cfc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1361577664.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-1361577664.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss:0.9246\n",
            "Epoch [2/10], Loss:0.9256\n",
            "Epoch [3/10], Loss:0.9281\n",
            "Epoch [4/10], Loss:0.9241\n",
            "Epoch [5/10], Loss:0.9301\n",
            "Epoch [6/10], Loss:0.9283\n",
            "Epoch [7/10], Loss:0.9255\n",
            "Epoch [8/10], Loss:0.9259\n",
            "Epoch [9/10], Loss:0.9232\n",
            "Epoch [10/10], Loss:0.9259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of the original and reconstructed images\n",
        "dataiter = iter(train_loader)\n",
        "images, _ = next(dataiter)\n",
        "images = images.to(device)\n",
        "\n",
        "# Display original images\n",
        "print(\"Original Images\")\n",
        "imshow(vutils.make_grid(images[:4], normalize=True))\n",
        "\n",
        "# Encoded and decoded images\n",
        "with torch.no_grad():\n",
        "    encoded_imgs = model.encoder(images.view(-1, 28*28)[:4])\n",
        "    decoded_imgs = model.decoder(encoded_imgs).view(-1, 1, 28, 28)\n",
        "\n",
        "\n",
        "# Display reconstructed images\n",
        "print(\"Original Representation\")\n",
        "print(img)\n",
        "\n",
        "# Print latent space\n",
        "print(\"Latent space representations:\")\n",
        "print(encoded_imgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1774
        },
        "id": "FCsJeFk2B2Fi",
        "outputId": "c5f4bc33-b713-40d2-8347-86dbdbce668b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Images\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Representation\n",
            "tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]]], device='cuda:0')\n",
            "Latent space representations:\n",
            "tensor([[ 0.0000, 36.8787, 14.5452,  0.0000,  0.0000, 19.2876,  0.0000, 26.2193,\n",
            "          0.0000, 22.4879, 15.3667, 22.2348, 15.2263,  0.0000,  0.0000,  0.0000,\n",
            "         16.2026, 18.6222, 25.2477,  0.0000,  0.0000, 34.2792, 14.1358, 11.0880,\n",
            "          0.0000,  0.0000, 20.5797,  0.0000, 25.7367, 20.9227, 34.8810, 21.3018,\n",
            "         35.7016, 18.9796, 12.6803,  0.0000,  0.0000, 38.8197,  0.0000,  0.0000,\n",
            "          0.0000, 31.4167,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         25.5371,  0.0000, 24.3165, 22.0938, 15.7670, 20.5714, 29.2549,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  7.2114,  0.0000,  0.0000,  0.0000, 20.7784],\n",
            "        [ 0.0000, 31.9996, 12.5971,  0.0000,  0.0000, 16.9673,  0.0000, 22.9119,\n",
            "          0.0000, 19.7336, 13.0462, 19.1570, 13.1414,  0.0000,  0.0000,  0.0000,\n",
            "         14.1863, 16.1093, 21.9688,  0.0000,  0.0000, 30.2127, 12.4355,  9.6412,\n",
            "          0.0000,  0.0000, 18.0252,  0.0000, 22.2435, 18.2813, 30.4479, 18.6235,\n",
            "         31.1857, 16.2387, 10.8527,  0.0000,  0.0000, 34.0336,  0.0000,  0.0000,\n",
            "          0.0000, 27.4543,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         22.4697,  0.0000, 21.1173, 19.2623, 13.6780, 18.1651, 25.4351,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  6.2782,  0.0000,  0.0000,  0.0000, 18.0007],\n",
            "        [ 0.0000, 29.5319, 11.8775,  0.0000,  0.0000, 15.5231,  0.0000, 21.1250,\n",
            "          0.0000, 18.3468, 12.0059, 17.8108, 11.8743,  0.0000,  0.0000,  0.0000,\n",
            "         13.2810, 15.0098, 20.0614,  0.0000,  0.0000, 27.7390, 11.5632,  8.7827,\n",
            "          0.0000,  0.0000, 16.4144,  0.0000, 20.5902, 16.7881, 27.8113, 17.0357,\n",
            "         28.6836, 14.8960,  9.9090,  0.0000,  0.0000, 31.6561,  0.0000,  0.0000,\n",
            "          0.0000, 25.3540,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         20.4351,  0.0000, 19.3179, 17.7818, 12.3704, 17.1952, 23.4062,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  5.8770,  0.0000,  0.0000,  0.0000, 16.6388],\n",
            "        [ 0.0000, 34.8779, 14.1931,  0.0000,  0.0000, 18.2819,  0.0000, 24.5706,\n",
            "          0.0000, 21.6692, 14.3557, 21.2289, 14.3991,  0.0000,  0.0000,  0.0000,\n",
            "         15.5324, 17.7112, 23.9846,  0.0000,  0.0000, 32.5118, 13.3064, 10.3476,\n",
            "          0.0000,  0.0000, 19.4618,  0.0000, 24.4724, 20.0635, 33.1188, 20.1597,\n",
            "         34.0384, 18.0849, 11.7530,  0.0000,  0.0000, 37.0561,  0.0000,  0.0000,\n",
            "          0.0000, 29.6075,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         24.1105,  0.0000, 23.0388, 20.9278, 14.6538, 19.7114, 27.6357,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  6.7961,  0.0000,  0.0000,  0.0000, 19.5962]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}